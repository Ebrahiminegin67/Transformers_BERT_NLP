# -*- coding: utf-8 -*-
"""Lab5_ML_NeginEbrahimi (4).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HNjYg0VZv5beCpnLJqsIAI23g-TERuyU

# Lab 5. Transformers

---

How to Use This Notebook
---

**Recommended Setup**
- For the best experience, **run this notebook on [Google Colab](https://colab.research.google.com/)**—especially if your local machine is slow.  
- In Colab, **enable GPU support** by going to:  
  `Runtime > Change runtime type > Hardware accelerator > GPU`


**Homework Tasks**

 - Homework tasks are clearly marked throughout the notebook in the following format:

   > ---

   > <span style="color:red"><b>TASK X</b> - [<i>some text</i>]:</span>

   > ---

   > ```Your code ....```

   > ---

   > *End of Task X.* [*Instructions for passing*]

 - For each task:
   - **Complete the code** where indicated.
   - **Upload the required results** from each task to **Homework 5 – Code** on [NextIlearn](https://nextilearn.dsv.su.se).

 - Once you've finished all the tasks:
   Submit your **entire completed notebook (including your code!)** to **Homework 5 – Notebook** on [NextIlearn](https://nextilearn.dsv.su.se).

**Important:**  
Your submission will **only be graded if both files** (code + notebook) are uploaded **before the deadline**. Late submissions are **not accepted**, regardless of technical issues like bad internet connection.

---

This lab introduces students to working with pre-trained BERT embeddings, fine-tuning BERT for classification, and extending BERT with a simple autoregressive head.

## Objectives
- Extract contextual embeddings using a pre-trained BERT model.
- Fine-tune BERT on a downstream classification task.
- Build a simple autoregressive extension of BERT.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # Install necessary libraries
# #!pip install transformers torch scikit-learn datasets matplotlib
# #!pip install -U datasets
# # Unzip data
# 
# !unzip data.zip

# Import required libraries
import torch
import numpy as np
import pandas as pd
from tqdm import tqdm
from datasets import load_dataset
from torch.utils.data import DataLoader
import torch.nn.functional as F

"""## 1. Using Pre-trained BERT Embeddings

Let's load a pre-trained BERT model and extract token-level and sentence-level embeddings. [Huggingface's `transformers` library](https://huggingface.co/transformers]) provides easy access to most open source transformer models. The most convenient way is using their `Auto`-classes, which work with most models:

* `transformers.AutoTokenizer` provides access to the pretrained tokenizer (i.e. including the vocabulary used for training the model)
* `transformers.AutoModel` provides access to the pretrained model in its base configuration (i.e. including the trained parameters)

Both provide the `.from_pretrained()` method, which automatically instantiates the correct class and downloads the pretrained data based on a model identifier:

*   `'bert-base-uncased'`: normal sized BERT ([Devlin et al, 2019](https://doi.org/10.48550/arXiv.1810.04805))
*   `'bert-small-uncased'`: a smaller version of BERT ([Devlin et al, 2019](https://doi.org/10.48550/arXiv.1810.04805))
*   `'FacebookAI/xlm-roberta-base'`: a BERT-sized multilingual model ([Conneau et al, 2019](https://doi.org/10.48550/arXiv.1911.02116))
*   `'gpt2'`: a predecessor of today's GPT-4o, the backbone of ChatGPT ([Radford et al, 2019](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf))

An extensive list of models can be found at [https://huggingface.co/models](https://huggingface.co/models).
"""

from transformers import AutoTokenizer, AutoModel

"""### **1. Step:** Load the pretrained tokenizer"""

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
type(tokenizer)

"""Let's encode some sample sentences to see how it works:"""

sentences = ["The quick brown fox jumps over the lazy dog.", "A stitch in time saves nine."]

# tokenize the sentences:
inputs = tokenizer(sentences,
  return_tensors='pt',    # return the output of this function as pytorch tensors.
                          # Other options: 'np' -> numpy
                          #                'tf' -> tensorflow

  padding='max_length',   # pad the sentences to context length of the model.
                          # Other options: 'longest' / True     -> pad to longest length in batch
                          #                'do_not_pad' / False -> no padding

  truncation=True         # Options: 'longest_first' / True    -> Truncate to a maximum length specified with the argument max_length or to the maximum acceptable input length for the model if that argument is not provided.
                          #          'do_not_truncate' / False -> No truncation (i.e., can output batch with sequence lengths greater than the model maximum admissible input size)
)

inputs

"""`inputs` is a dictionary-like object including all the information needed for the transformer:

#### **Member `'input_ids'`**: the actual tokens!
"""

# in encoded form:
inputs['input_ids']

# the shape is number of input texts x sequnece length:
inputs['input_ids'].shape
# 2 is the number of sentences we had

# can be converted back to text:
[tokenizer.decode(ids) for ids in inputs['input_ids']]
# you get the original sentence after decoding so it works

"""[huggingface tokenizer objects](https://huggingface.co/docs/transformers/main_classes/tokenizer) contain information on the model specific special tokens:"""

tokenizer.special_tokens_map

# Unknown token -> Encodes tokens that have not occured in the training data
tokenizer.unk_token, tokenizer.unk_token_id

# classification token -> Starts a sequence. Because of this, other transformers
# usually use "beginning of sequence" ([bos]) instead. For BERT-like models
# it corresponds to the position of the classification output.
tokenizer.cls_token, tokenizer.cls_token_id

# Separator token -> Separates two sentences for the next sentence prediction task
# after pretraining usually used to end the input sequence. Because of this,
# other transformers usually use "end of sequence" ([eos]) instead.
tokenizer.sep_token, tokenizer.sep_token_id

# Padding token -> Pads sequences to the full input length of the transformer.
tokenizer.pad_token, tokenizer.pad_token_id

# Mask token -> For the masked language modelling pretraining task. Rarelly used
# after pretraining.
tokenizer.mask_token, tokenizer.mask_token_id

"""#### **Member `'attention_mask'`**: a mask specifing the position of  non-padded input tokens in `'input_ids'`!"""

inputs['attention_mask']

inputs['attention_mask'].shape
# again 2 for the number of sentences

[tokenizer.decode(ids[mask == 1]) for ids, mask in zip(inputs['input_ids'], inputs['attention_mask'])]

[tokenizer.decode(ids[mask == 0]) for ids, mask in zip(inputs['input_ids'], inputs['attention_mask'])]

"""#### **Member `'token_type_ids'`**: a mask specifing the position of the two sentences in `'input_ids'` for the next sentence prediction pretraining task!

*Usually not needed after pretraining!*

"""

inputs['token_type_ids']

inputs['token_type_ids'].shape

"""### **2. Step:** Load the pretrained model"""

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device

# load model:
model = AutoModel.from_pretrained('bert-base-uncased')
model.to(device)
model.eval()

model

"""The `config` property contains information about the model instance:"""

model.config
#configuration we use to go from a general transformer layout to a specific layout you are using

"""We can use this model rightaway, since it is already pretrained:"""

with torch.no_grad():
  outputs = model(
      input_ids=inputs['input_ids'].to(device),
      attention_mask=inputs['attention_mask'].to(device),
      output_hidden_states=True,                # return the hidden states after each transformer layer (default: False)
      output_attentions=True                    # return the self-attention weights (default: False)
  )
outputs.keys()

"""`outputs` is again a dictionary-like object:

#### **Member `'last_hidden_state'`**: the output of the last self-attention layer (i.e. before pooling)!
"""

outputs['last_hidden_state']
# the output before the pooler

outputs['last_hidden_state'].shape
#2 sentences , 512 size of the input , and the 768 is the output which is the
#length of each embedding vector

"""#### **Member `'pooler_output'`**: the output of the pooling layer!

The pooler is a single linear layer with a tanh activation:

```Python
class BertPooler(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.activation = nn.Tanh()
    
    def forward(self, hidden_states):
        # We "pool" the model by simply taking the hidden state corresponding
        # to the first token.
        first_token_tensor = hidden_states[:, 0]
        pooled_output = self.dense(first_token_tensor)
        pooled_output = self.activation(pooled_output)
        return pooled_output
```

This layer recieves the last hidden state corresponding to the `[CLS]` token as the input. Remember that the next sentence prediction task is trained on this output for BERT.

(see [BERT source code](https://huggingface.co/transformers/v3.0.2/_modules/transformers/modeling_bert.html))
"""

outputs['pooler_output']

# the outputs of each single transformer layer  (hidden)

outputs['pooler_output'].shape
# this time we only length of the sentence and the vector

"""#### **Member `'hidden_states'`**: all the hidden states after each transformer layer!

*This is only returned because we specified* `output_hidden_states=True`*!*
"""

type(outputs['hidden_states']), len(outputs['hidden_states'])

[t.shape for t in outputs['hidden_states']]

"""#### **Member `'attentions'`**: all the attention weights of each head in each transformer layer!

*This is only returned because we specified* `output_attentions=True`*!*
"""

type(outputs['attentions']), len(outputs['attentions'])

[t.shape for t in outputs['attentions']]
#12 layers with 12 attention heads each

"""Let's have a look at the average attention in the last layer:"""

import matplotlib.pyplot as plt

for i in range(2):
  fig, axs = plt.subplots(ncols=12, figsize=(20, 5))
  for layer, ax in enumerate(axs):
    # get attention weights of last transformer layer:
    aw = outputs['attentions'][layer][i].cpu()

    # average over heads:
    aw = aw.mean(dim=0)

    # remove padding tokens:
    mask = inputs['attention_mask'][i]
    aw = aw[mask == 1, :][:, mask == 1]

    # create labels:
    labels = tokenizer.convert_ids_to_tokens(inputs['input_ids'][i][mask == 1])
    x = np.arange(len(labels))

    ax.imshow(aw.detach().numpy())
    ax.set_xticks(ticks=x, labels=labels, rotation=90)
    ax.set_yticks(ticks=x, labels=['']*len(x))
    ax.set_title(f'Layer {layer+1}')

  axs[0].set_yticks(ticks=x, labels=labels)
  plt.tight_layout()
  plt.show()

  #this shows how importent tokens are in each layer
  #each row represents a sentence

"""Note how every attention layer focuses on different token combinations.

### Example: using pretrained models as embeddings without furter fine-tuning

Let's load a dataset:

From (https://paperswithcode.com/dataset/ag-news)[https://paperswithcode.com/dataset/ag-news]:
> *AG News (AG’s News Corpus) is a subdataset of AG's corpus of news articles constructed by assembling titles and description fields of articles from the 4 largest classes (“World”, “Sports”, “Business”, “Sci/Tech”) of AG’s Corpus. The AG News contains 30,000 training and 1,900 test samples per class.*
"""

from datasets import load_dataset


labels = ["World", "Sports", "Business", "Sci/Tech"]
data = load_dataset("ag_news", split='train[:2000]')
data

data[0]

"""Add a tokenization step to the pipeline:"""

def tokenize_function(example):
    return tokenizer(example['text'], truncation=True, padding='max_length')

data = data.map(tokenize_function, batched=True)
data

"""Hide irrelevant features:"""

data.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])
data[0]

"""Calculate BERT embeddings:"""

x, y = [], []

# predict:
for batch in tqdm(DataLoader(data, batch_size=32)):
  with torch.no_grad():
    y.extend(batch.pop('label').cpu().numpy())

    batch = {k: v.to(device) for k, v in batch.items()}
    x.extend(model(**batch).pooler_output.cpu().numpy())

# convert lists to numpy:
x = np.array(x)
y = np.array(y)

x.shape, y.shape

from sklearn.model_selection import ShuffleSplit
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import f1_score

f1_knn, f1_rnd = [], []
ks = np.arange(1, 21)
for k in tqdm(ks):
  f1_knn.append([])
  f1_rnd.append([])

  # Monte-Carlo cross-validation with 50 splits:
  mc = ShuffleSplit(n_splits=50, test_size=0.25, train_size=None)
  for idx_train, idx_test in mc.split(x):

    # knn classifier:
    knn = KNeighborsClassifier(n_neighbors=k).fit(x[idx_train], y[idx_train])
    y_pred = knn.predict(x[idx_test])
    f1_knn[-1].append(f1_score(y[idx_test], y_pred, average='macro'))

    # random baseline:
    y_rnd = np.random.randint(0, len(labels), size=len(y[idx_test]))
    f1_rnd[-1].append(f1_score(y[idx_test], y_rnd, average='macro'))

# convert to numpy:
f1_knn = np.array(f1_knn)
f1_rnd = np.array(f1_rnd)

# plot:
plt.plot(np.mean(f1_knn, axis=1), label='KNN')
plt.plot(np.mean(f1_rnd, axis=1), label='Random')
plt.legend()
plt.xticks(ticks=ks-1, labels=ks)
plt.xlabel('$k$')
plt.ylabel('F$_1$-score')
plt.show()

#simple classifier has a way much better performance than randome guessing

from typing import Iterable
from transformers import BertModel, BertTokenizer
from numpy.typing import NDArray
from sklearn.metrics.pairwise import cosine_similarity

"""---

<span style="color:red"><b>TASK 1</b> - Text similarity:</span>

---

Write a function to compute text similarity using cosine similarity between BERT embeddings of different texts. The function should take a list of $n$ strings as its input and return a similarity matrix $\in \mathbb{R}^{n \times n}$.

Afterward, use this function to compute the similarity for the sentences in `data.zip/task1/sentences.csv` and upload your solution to NextIlearn.

**Hint:** See [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html) for documentation of the `sklearn.metrics.pairwise.cosine_similarity` function.
"""

!unzip data.zip

import pandas as pd
sentences_df = pd.read_csv("data/task1/sentences.csv", index_col=0)
texts = list(sentences_df["sentences"])

from transformers import BertTokenizer, BertModel
import torch
from sklearn.metrics.pairwise import cosine_similarity

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
model = BertModel.from_pretrained("bert-base-uncased")
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)


def compute_text_similarity(
    input_texts: Iterable[str], transformer_model: BertModel, tokenizer: BertTokenizer, device: torch.device) -> NDArray[np.float32]:
    batch = tokenizer( list(input_texts), padding=True, truncation=True, max_length=128, return_tensors="pt").to(device)

    with torch.no_grad():
        outputs = transformer_model(**batch)

    cls_embeddings = outputs.pooler_output.detach().cpu().numpy()

    similarity_matrix = cosine_similarity(cls_embeddings).astype(np.float32)
    return similarity_matrix

sentences  = pd.read_csv('data/task1/sentences.csv', index_col=0)
similarity = compute_text_similarity(sentences['sentences'].values.tolist(), model, tokenizer, device)
pd.DataFrame(similarity).to_csv('similarity.csv')

"""---

*End of Task 1. Upload your final predictions (the file* `similarity.csv` *) to* **Homework 2 - Code** *on* **NextIlearn**

When you are done with Task 1, feel free to play around with the function a little:
"""

plt.imshow(compute_text_similarity(['Paul is cooking dinner for his friend.', 'Maria is cooking dinner for her friend.', 'Stockholm is a beautiful city!'], model, tokenizer, device))

"""## 2. Fine-tuning BERT for Text Classification

While the KNN classifier based on BERT embeddings is already performing well, we can improve on them by fine-tuning the model for our task. We will reuse the data preprocessing pipeline we used before!
"""

# split data in training and test set:
split_data = data.train_test_split(test_size=0.25)
split_data

# create dataloaders:
train_dataloader = DataLoader(split_data['train'], batch_size=8)
test_dataloader = DataLoader(split_data['test'], batch_size=8)

"""For classification, we need a different "head" on our model. Huggingface provides different setups for each model (see [huggingface documentation](https://huggingface.co/transformers/v3.0.2/model_doc/auto.html)):

*   `transformers.AutoModel`
*   `transformers.AutoModelForPreTraining`
*   `transformers.AutoModelWithLMHead`
*   `transformers.AutoModelForSequenceClassification`
*   `transformers.AutoModelForQuestionAnswering`
*   `transformers.AutoModelForTokenClassification`

For text classification we use `transformers.AutoModelForSequenceClassification`:
"""

from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=len(labels))
model = model.to(device)
model

"""As you can see, this model has an **additional linear layer** after the pooler layer. It also comes with built-in loss calculation for convenience:

### These loss functions are automatically selected by the model we don't select them

```Python
loss = None
if labels is not None:
    if self.config.problem_type is None:
        if self.num_labels == 1:
            self.config.problem_type = "regression"
        elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):
            self.config.problem_type = "single_label_classification"
        else:
            self.config.problem_type = "multi_label_classification"

    if self.config.problem_type == "regression":
        loss_fct = MSELoss()
        if self.num_labels == 1:
            loss = loss_fct(logits.squeeze(), labels.squeeze())
        else:
            loss = loss_fct(logits, labels)
    elif self.config.problem_type == "single_label_classification":
        loss_fct = CrossEntropyLoss()
        loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
    elif self.config.problem_type == "multi_label_classification":
        loss_fct = BCEWithLogitsLoss()
        loss = loss_fct(logits, labels)
```
(*from* [https://github.com/huggingface/transformers/blob/v4.51.3/src/transformers/models/bert/modeling_bert.py](https://github.com/huggingface/transformers/blob/v4.51.3/src/transformers/models/bert/modeling_bert.py#L1692))


Let's try this out:
"""

model(
  input_ids      = data[:1]['input_ids'].to(device),
  attention_mask = data[:1]['attention_mask'].to(device),
  labels         = data[:1]['label'].to(device)
)

"""Now let's train this model. Remember, that **in fine-tuning we want to update the pretrained weights, not re-train the model**. Therefore, we train the model for a low number of epochs with a low learning rate. The original BERT paper proposes the following configuration ([Devlin et al, 2019](https://doi.org/10.48550/arXiv.1810.04805)):

* **Learning rate with Adam:** *5e-5, 3e-5, 2e-5*
* **Number of epochs:** *2, 3, 4*

Further reading: *Huggingface also provides a* `transformers.Trainer`*-class that you may want to try out for convenience: [https://huggingface.co/docs/transformers/main_classes/trainer](https://huggingface.co/docs/transformers/main_classes/trainer)*
"""

from torch.optim import AdamW

# Optimizer: and we are updating the weight somehow here
optimizer = AdamW(model.parameters(), lr=5e-5)

# Training loop:
loss_train = []
for epoch in range(2):

  model.train()
  loss_train.append([])
  for batch in train_dataloader:
    optimizer.zero_grad()

    outputs = model(
      input_ids      = batch['input_ids'].to(device),
      attention_mask = batch['attention_mask'].to(device),
      labels         = batch['label'].to(device)
    )

    loss = outputs.loss
    loss.backward()
    loss_train[-1].append(loss.item())

    optimizer.step()
  loss_train[-1] = np.mean(loss_train[-1])

  print(f"Epoch {epoch+1}: loss = {loss_train[-1]:.3f}")

# Evaluation:
model.eval()
y_true, y_pred = [], []
for batch in test_dataloader:
  with torch.no_grad():
    outputs = model(
      input_ids      = batch['input_ids'].to(device),
      attention_mask = batch['attention_mask'].to(device)
    )

  y_pred.extend(outputs.logits.argmax(dim=1).cpu().numpy())
  y_true.extend(batch['label'].cpu().numpy())

f1_score(y_true, y_pred, average='macro')

"""---

<span style="color:red"><b>TASK 2</b> - Fine-Tuning RoBERTa:</span>

---

Fine-tune [RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta) a pretrained transformer model to classify the data you received along with this notebook (in `data.zip/task2/...`). **You are enouraged to use scheduling and early stopping, but remember to keep the learning rate low.**

**Upload the resulting predictions to NextIlearn. Your model should achieve an F$_1$ > .77 to pass.**
"""

import pandas as pd

# Load training metadata and construct full paths to text files
train_metadata = pd.read_csv("data/task2/train/labels.csv", index_col=0)
train_metadata["filepath"] = train_metadata["file"].apply(lambda name: f"data/task2/train/{name}")

# Display class distribution
positive_count = (train_metadata.label == 1).sum()
negative_count = (train_metadata.label == 0).sum()
print(f"Positive samples: {positive_count}")
print(f"Negative samples: {negative_count}")

import os
test_files = ['data/task2/test/' + s for s in os.listdir('data/task2/test/')]
test_files.sort()
test_files = pd.DataFrame({'file': test_files})
test_files.head()

from transformers import RobertaTokenizer

roberta_tokenizer = RobertaTokenizer.from_pretrained("roberta-base")

def tokenize_text_file(file_path: str):
    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()

    return roberta_tokenizer(
        content,
        padding="max_length",
        truncation=True,
        max_length=128,
        return_tensors="pt"
    )

from torch.utils.data import Dataset
import torch

class RobertaTrainDataset(Dataset):
    def __init__(self, dataframe):
        self.data = dataframe

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        file_path = self.data.iloc[index]["filepath"]
        label = self.data.iloc[index]["label"]
        tokens = tokenize_text_file(file_path)

        return {
            "input_ids": tokens["input_ids"].squeeze(),
            "attention_mask": tokens["attention_mask"].squeeze(),
            "label": torch.tensor(label, dtype=torch.long)
        }

from torch.utils.data import DataLoader
from sklearn.utils.class_weight import compute_class_weight
import numpy as np

train_data = RobertaTrainDataset(train_metadata)
train_loader = DataLoader(train_data, batch_size=32, shuffle=True)

# Calculate class weights for imbalanced data
weights = compute_class_weight(
    class_weight="balanced",
    classes=np.array([0, 1]),
    y=train_metadata["label"]
)
weights_tensor = torch.tensor(weights, dtype=torch.float32).to(device)

from transformers import RobertaForSequenceClassification
from torch.nn import CrossEntropyLoss
from torch.optim import AdamW

# Load RoBERTa model for binary classification
classifier = RobertaForSequenceClassification.from_pretrained("roberta-base", num_labels=2)
classifier.to(device)

optimizer = AdamW(classifier.parameters(), lr=5e-6)
loss_fn = CrossEntropyLoss(weight=weights_tensor)

classifier.train()
all_losses = []

for epoch in range(2):
    epoch_losses = []

    for batch in train_loader:
        optimizer.zero_grad()
        ids = batch["input_ids"].to(device)
        masks = batch["attention_mask"].to(device)
        labels = batch["label"].to(device)

        results = classifier(input_ids=ids, attention_mask=masks)
        loss = loss_fn(results.logits, labels)
        loss.backward()
        optimizer.step()
        epoch_losses.append(loss.item())

    mean_loss = np.mean(epoch_losses)
    all_losses.append(mean_loss)
    print(f"Epoch {epoch + 1} | Loss: {mean_loss:.4f}")

import os
import pandas as pd
from torch.utils.data import Dataset, DataLoader
import torch

# Step 1: Load and organize test file paths
test_dir = "data/task2/test"

test_metadata = pd.DataFrame({
    "filepath": [
        os.path.join(test_dir, fname)
        for fname in os.listdir(test_dir)
        if fname.endswith(".txt")
    ]
}).sort_values("filepath").reset_index(drop=True)

# Step 2: Custom dataset for inference
class RobertaTestDataset(Dataset):
    def __init__(self, dataframe):
        self.data = dataframe

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        path = self.data.iloc[index]["filepath"]
        tokens = tokenize_text_file(path)
        return {
            "input_ids": tokens["input_ids"].squeeze(),
            "attention_mask": tokens["attention_mask"].squeeze()
        }

# Step 3: Create DataLoader
test_data = RobertaTestDataset(test_metadata)
test_loader = DataLoader(test_data, batch_size=8)

# Step 4: Make predictions
classifier.eval()
final_preds = []

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)

        outputs = classifier(input_ids=input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()
        final_preds.extend(preds)

# Step 5: Save predictions to CSV
submission_df = pd.DataFrame(final_preds, columns=["predictions"])
submission_df.to_csv("submission.csv", index=False)

"""---

*End of Task 2. Upload your final predictions (the file* `submission.csv` *) to* **Homework 2 - Code** *on* **NextIlearn**

## 3. Simple Autoregressive Extension of BERT

BERT is not autoregressive by design. Here, we try to simulate next-token prediction using BERT with causal masking.

Remember, that in the MLM pretraining task, BERT is trained to predict the most probable token corresponding to the input token `'[MASK]'`.

E.g.: `'[CLS] The weather is [MASK]. [SEP]'` → **BERT** → `'[CLS] The weather is great. [SEP]'`

We will use this to simulate autoregression, i.e. the generation of a text token by token.
"""

from transformers import AutoModelForMaskedLM
model = AutoModelForMaskedLM.from_pretrained('bert-base-uncased')
model = model.to(device)
model.eval()
model

# as you notice here we don't have a pooling layer beacuse we do not do any predictions

"""An example:"""

sentence = "The capital of France is [MASK]."

# tokenize:
inputs = tokenizer(sentence, return_tensors="pt").to(device)

# generate:
with torch.no_grad():
    logits = model(**inputs).logits

logits.shape

# the result is a probability distribution 1 for the sentence , 9 for the tokens and 30522
# is the size of the tokens in the vocabulary .
#logit is the probability for the token to be the one

"""The shape of `logits` is:

  *number of texts* $~\times~$ *number of tokens*  $~\times~$  *vocabulary size*

→ This is a collection of token probabilities!
"""

# get top prediction for masked token:
predicted_token_id = logits[0, -3].argmax(axis=-1)

print("Predicted token:", tokenizer.decode(predicted_token_id))

"""Now let's make this a function:"""

#Auto regressive language modeling
from typing import Optional

def complete_text(prompt:str, max_tokens:Optional[int]=None, model=model, tokenizer=tokenizer, device=device):
  # use the whole context window if max_tokens not specified:
  if max_tokens is None: max_tokens = tokenizer.model_max_length - len(tokenizer(prompt).input_ids)

  # pad prompt with '[MASK]' tokens to tell BERT the number of tokens:
  prompt += ' '.join(['[MASK]']*max_tokens)

  # tokenize:
  inputs = tokenizer(prompt, return_tensors="pt").to(device)

  # generate token probabilities:
  with torch.no_grad():
    logits = model(**inputs).logits

  # get top prediction for first masked token in the input sequence:
  predicted_token_id = logits[0, -max_tokens-1].argmax(axis=-1).cpu().tolist()
# if we raech the max we end the regression
  text = tokenizer.decode(inputs.input_ids[0, 1:-max_tokens-1].cpu().tolist() + [predicted_token_id])

  # end autoregression if max_tokens == 1:
  if max_tokens == 1: return text

  # end autoregression on '.' token:
  if predicted_token_id == tokenizer.vocab['.']: return text

  # end autoregression on [SEP] token:
  if predicted_token_id == tokenizer.sep_token_id: return text

  return complete_text(text, max_tokens=max_tokens-1, model=model, tokenizer=tokenizer, device=device)

"""Let's see if it works:

this is the different token selection strategies , greedy , beam ...
"""

complete_text("The capital of France is ", max_tokens=2)

complete_text("The capital of France is ", max_tokens=100)

# higher max tokens leads the model to get in a loop of repeating the last token

"""Another example:"""

complete_text("one plus one is equal to ", max_tokens=2)

complete_text("one plus one is equal to ")
# without max tokens it finish the sentence with somthing that doesn't make sense